Realtime API モバイル音声誤認識問題まとめ

1. 問題概要

モバイルアプリに Realtime API を導入した際に以下の問題が発生：
• 話していないのに発話として認識される
• AIの音声をマイクが拾ってしまう
• AIがその誤認識に応答する
• 応答音声を再度マイクが拾う
• 無限ループが発生する
• 会話が成立しない

⸻

2. 根本原因

この問題は モデル精度の問題ではなく、音声制御設計の問題 である。

主な原因は以下の通り。

2.1 エコー（TTS自己拾い）
• AIの音声出力をマイクが拾う
• 再入力として処理される
• 自己会話ループが発生する

2.2 VAD（Voice Activity Detection）の過敏反応

以下を発話として誤検知する可能性がある：
• 環境ノイズ
• 呼吸音
• 衣擦れ
• 端末操作音
• エアコン音

2.3 発話終了判定が短すぎる
• 自然な「間」を発話終了と誤認識
• 推論が早すぎる
• AIが不必要に応答する

2.4 入力フィルタ未実装
• 短すぎるテキストを無条件で処理
• 信頼度チェックなし
• ノイズでも応答生成

⸻

3. ネイティブアプリとの差

ネイティブアプリは API を単純利用しているわけではない。

3.1 高度な音響前処理が存在
• AEC（Acoustic Echo Cancellation）
• AGC（Auto Gain Control）
• ノイズサプレッション
• ビームフォーミング
• 高精度VAD

3.2 動的マイク制御
• AI発話中にマイク感度を下げる
• TTS波形と一致する音を除去

3.3 入力フィルタリング
• 低confidence破棄
• 極短文無視
• ノイズパターン除外

API単体ではこれらは提供されない。

⸻

4. 推奨対策（優先順位順）

4.1 最優先対策1. AI発話中はマイク入力を無効化2. 1秒未満の音声入力は破棄3. 3文字未満のテキストは無視4. 直前がAI発話なら一定時間入力無視（例：1.5秒）

⸻

4.2 音響レイヤー対策

iOS
• AVAudioSessionCategoryPlayAndRecord 使用
• AEC 有効化
• ノイズサプレッション有効化

Android
• AudioRecord で AEC 有効
• NoiseSuppressor 有効
• AutomaticGainControl 有効

⸻

4.3 発話終了タイミング調整
• 無音タイムアウトを 800ms〜1200ms に設定
• 早すぎる推論開始を防止

⸻

4.4 入力バリデーション例

if (text.length < 3) ignore
if (confidence < threshold) ignore
if (assistant_spoke_recently) ignore

⸻

5. 重要な設計原則

原則1

音声AIの難易度の8割は音響I/O設計

原則2

モデルは「入力を信じる」ため、入力品質がすべて

原則3

Realtime APIは体験エンジンではなく推論エンジン

⸻

6. まとめ

今回の誤認識問題は：
• モデル精度の問題ではない
• APIの問題でもない
• 音声制御レイヤー未設計による問題

安定させるには：
• エコー対策
• マイク制御
• 入力フィルタ
• 発話終了調整

を段階的に実装する必要がある。

⸻

7. 次ステップ
   • 現在の端末（iOS / Android）を明確化
   • スピーカー前提かイヤホン前提かを決定
   • 音響制御レイヤー設計を先に固める
   • その後モデル最適化を検討する

⸻

この問題は音声AI開発における典型的な初期壁であり、適切な音響制御を実装すれば解決可能である。

:::

realtime api モバイルアプリとして導入してみたんだけど誤認識されてしまって会話が進まない
これはどうやって対応しているのだろうか
話してもいないのに何かを認識して話したことになってしまいループしてしまう

それはRealtime系あるあるの一番キツい問題だね。

🎤 話してないのに認識される
🔁 それに反応してAIが喋る
🎤 その音をまた拾う
🔁 無限ループ

これは モデルの問題というより、音声制御設計の問題。

順番に分解するね。

⸻

① なぜ起きるのか？

主な原因は4つ。

⸻

1. エコー（TTSを自分で拾う）

AIの音声出力をマイクが拾ってしまう。

ユーザー無言
↓
AIが少しノイズを誤認識
↓
AIが返答
↓
その音声を再度マイクが拾う
↓
会話継続と判断
↓
無限ループ

📱 モバイルだとこれが一番多い。

⸻

2. VAD（Voice Activity Detection）が敏感すぎる

Realtimeは内部で発話検知をしているけど、
• エアコン音
• 衣擦れ
• 呼吸
• スマホを触る音

これを「発話」と誤認識することがある。

⸻

3. 無音タイムアウト設定が短すぎる
   • ちょっと間が空く
   • 「発話終了」と判断
   • 推論開始
   • AIが喋る

自然な会話の「間」がノイズ扱いされる。

⸻

4. 自動応答ルールが甘い

例えば：
• 何かテキストが来たら必ず返す
• confidenceチェックしていない

これだとゴミ入力にも反応する。

⸻

② 実際のプロダクトはどうしている？

ちゃんとした音声AIはこうしてる：

⸻

✅ 1. エコーキャンセル必須

iOSなら：
• AVAudioSessionCategoryPlayAndRecord
• AEC（Acoustic Echo Cancellation）ON

Androidなら：
• AudioRecordでAEC有効化
• ノイズサプレッションON

これをやらないとほぼ破綻する。

⸻

✅ 2. 「AI発話中はマイクを止める」

これが一番シンプルで強い。

AI speaking = true
→ マイク入力を無視

これだけでループはほぼ止まる。

自然さは少し落ちるけど安定する。

⸻

✅ 3. 発話確信度チェック

Realtimeから返るtranscriptに：
• 信頼度
• 長さ
• 実際の文字数

これを見て：

if (text.length < 3) ignore
if (confidence < threshold) ignore

とする。

「はい」とか「うん」は通すけど、
「…」みたいなゴミは弾く。

⸻

✅ 4. サイレンス・バッファを入れる

発話終了判定を：
• 300ms → NG
• 800〜1200ms → 安定

少し長めに取る。

⸻

✅ 5. 連続自己発話ガード

例えば：

if (last_speaker == "assistant" &&
new_input_within < 1.5sec)
ignore

これだけでかなり安定する。

⸻

③ 実践的な対策（おすすめ順）

★最優先1. AIが喋ってる間はマイク無視2. 1秒未満の入力は無視3. 3文字未満は無視

これでループは止まる可能性高い。

⸻

④ モデル側で解決できる？

正直に言うと：

ほぼできない。

これは音響処理の問題。

モデルは「入力された音声」を信じるしかない。

⸻

⑤ さらに高度な方法

ちゃんと作るなら：
• 入力波形の音量レベル監視
• ノイズフロア計測
• 音圧が一定以下なら破棄
• 周波数帯域で人声か判定

ここまでやるとプロ仕様。

⸻

⑥ 重要な質問

あなたのアプリは：
• 🎧 スピーカー再生？
• 🎧 イヤホン前提？

イヤホン前提なら一気に安定する。

スピーカー前提なら、
音響制御を本気でやらないと厳しい。

⸻

⑦ 本音

音声AIで一番難しいのは

❌ LLM精度
⭕ 音声I/O制御

ここが設計の8割。

⸻

もしよければ：
• iOS？Android？
• スピーカー？イヤホン？
• 今どんな構成？

教えてくれれば、具体的な実装レベルで詰められる。

open aiのネイティブのアプリでは会話がスムーズにできるのになんでapiでは正確じゃないのか

めちゃくちゃ本質的な質問。

結論から言うと：

モデルの精度差ではなく、アプリ側の音声制御スタックの差

ネイティブアプリは
「APIを叩いているだけ」ではない。

⸻

まず前提

OpenAI のネイティブアプリも
内部的には Realtime系モデル を使っている。

つまり：

モデル自体が特別に高精度というわけではない

じゃあ何が違うのか？

⸻

決定的な違い

① 音声前処理レイヤーが全然違う

ネイティブアプリには：
• 高精度VAD（発話検知）
• ノイズプロファイル学習
• エコーキャンセル（AEC）
• 自動ゲイン制御（AGC）
• ビームフォーミング（端末依存）

が入っている。

あなたのAPI実装は：

マイク入力 → そのまま送信

になっていない？

ここが最大の差。

⸻

② マイク制御の精密さ

ネイティブアプリは：

AIが話す瞬間 → マイク感度を動的に下げる

あるいは：

TTS出力の波形と一致する音を除去

をしている。

これはアプリレベルの音響処理。

API単体ではやってくれない。

⸻

③ 発話確信度フィルタ

ネイティブは：
• 低confidenceは破棄
• 極短文は無視
• ノイズパターンはブラックリスト

をやっている可能性が高い。

APIは：

「受け取った音声は正しい前提」

で推論する。

⸻

④ セッション内部制御

ネイティブアプリでは：
• 応答タイミングを微調整
• 発話終了判定を遅らせる
• 自己発話ループを検知

などのガードがある。

API利用側はこれを自分で実装する必要がある。

⸻

⑤ モデルの微差はある？

正直に言うと：
• ネイティブは最適化ビルド版を使っている可能性はある
• でも体感差の90%は音響制御

モデルの知能差ではない。

⸻

図で表すと

ネイティブ

マイク
↓
高度な音響処理層
↓
Realtimeモデル
↓
出力制御層
↓
スピーカー

API直利用

マイク
↓
Realtimeモデル
↓
スピーカー

この差。

⸻

じゃあどうすれば近づく？

最低限やるべきこと：

① AI発話中はマイク無効化

② 1秒未満入力は破棄

③ 3文字未満は無視

④ iOS/AndroidのAEC有効化

⑤ AGC有効化

これだけでかなり近づく。

⸻

さらに重要な視点

ネイティブアプリは

「会話体験」を作っている

APIは

「モデルアクセス」を提供している

体験設計はあなたの仕事になる。

⸻

本音

多くの人が

「API精度が悪い」

と思うけど、

実際は

「音響エンジニアリングが足りない」

ケースがほぼ全て。

⸻

聞きたい：
• 今スピーカー再生？
• イヤホン？
• iOS？Android？

ここで対策レベルが変わる。
